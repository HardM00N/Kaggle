{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Contents\n3. Feature engineering\n- 3.1 Fill Null data\n- 3.1.1 Fill Null in Age using title\n- 3.1.2 Fill Null in Embarked\n- 3.2 Change Age (continuous to categorical)\n- 3.3 Change Initial, Embarked and Sex (string to numerical)\n- 3.4 One-hot encoding on Initial and Embarked\n- 3.5 Drop columns\n4. Building machine learning model and prediction using the trained model\n- 4.1 Preparation - Split dataset into train, valid and test set\n- 4.2 Model generation and prediction\n- 4.3 Feature importance\n5. Conclusion","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas import Series\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn')\nsns.set(font_scale=2.5)\n# 위 두 줄은 본 필자가 항상 쓰는 방법이다. matplotlib의 기본 scheme 말고 seaborn scheme을 세팅하고, 일일이 graph의 font size를 지정할 필요 없이 seaborn의 font_scale을 사용하면 된다. \n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\ndf_train = pd.read_csv('../input/titanic/train.csv')\ndf_test = pd.read_csv('../input/titanic/test.csv')\ndf_train['FamilySize'] = df_train['SibSp'] + df_train['Parch'] + 1  # 자신을 포함해야 하므로 1을 더한다.\ndf_test['FamilySize'] = df_test['SibSp'] + df_test['Parch'] + 1. # 자신을 포함해야 하므로 1을 더한다.\n\ndf_test.loc[df_test.Fare.isnull(), 'Fare'] = df_test['Fare'].mean()\n\ndf_train['Fare'] = df_train['Fare'].map(lambda i: np.log(i) if i > 0 else 0)\ndf_test['Fare'] = df_test['Fare'].map(lambda i: np.log(i) if i > 0 else 0)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:19.465595Z","iopub.execute_input":"2022-01-21T16:07:19.466106Z","iopub.status.idle":"2022-01-21T16:07:20.605999Z","shell.execute_reply.started":"2022-01-21T16:07:19.466011Z","shell.execute_reply":"2022-01-21T16:07:20.605242Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 3. Feature engineering\n- 본격적인 feature engineering을 시작해보자. \n- 가장 먼저, dataset에 존재하는 null data를 채우려고 한다. \n- 아무 숫자로 채울 수는 없고, null data를 포함하는 feature의 statistics를 참고하거나, 다른 아이디어를 짜내어 채울 수 있다. \n- null data를 어떻게 채우느냐에 따라 모델의 성능이 좌지우지될 수 있기 때문에, 신경써야할 부분이다. \n- feature engineering은 실제 모델의 학습에 쓰려고 하는 것이므로, train뿐만 아니라 test도 똑같이 적용해주어야 한다. 잊지 말자.","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Fill Null\n### 3.1.1 Fill Null in Age using title\n- Age에는 null data가 177개나 있다. 이를 채울 수 있는 여러 아이디어가 있을 것인데, 여기서 우리는 title + statistics를 사용해보자.\n- 영어에서는 Miss, Mrr, Mrs 같은 title이 존재한다. 각 탑승객의 이름에는 꼭 이런 title이 들어가게 되는데 이를 사용해보자. \n- pandas series에는 data를 string으로 바꿔주는 str method, 거기에 정규표현식을 적용하게 해주는 extract method가 있다. 이를 사용해 title을 쉽게 추출할 수 있다. title을 Initial column에 저장하자. ","metadata":{}},{"cell_type":"code","source":"df_train['Initial'] = df_train.Name.str.extract('([A-Za-z]+)\\.')  # lets extract the Salutations\ndf_test['Initial'] = df_test.Name.str.extract('([A-Za-z]+)\\.')  # lets extract the Salutations","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:20.608051Z","iopub.execute_input":"2022-01-21T16:07:20.608636Z","iopub.status.idle":"2022-01-21T16:07:20.620850Z","shell.execute_reply.started":"2022-01-21T16:07:20.608590Z","shell.execute_reply":"2022-01-21T16:07:20.620060Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"- pandas의 crosstab을 이용해 우리가 추출한 Initial과 Sex 간의 count를 살펴보자. ","metadata":{}},{"cell_type":"code","source":"pd.crosstab(df_train['Initial'], df_train['Sex']).T.style.background_gradient(cmap='summer_r')  # Checking the Initials with the Sex","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:20.622382Z","iopub.execute_input":"2022-01-21T16:07:20.623396Z","iopub.status.idle":"2022-01-21T16:07:20.737200Z","shell.execute_reply.started":"2022-01-21T16:07:20.623348Z","shell.execute_reply":"2022-01-21T16:07:20.736283Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"- 위 table을 참고해 남자, 여자가 쓰는 initial을 구분해보자. replace 메소드를 사용하면, 특정 데이터 값을 원하는 값으로 치환해준다. ","metadata":{}},{"cell_type":"code","source":"df_train['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don','Dona'],\n                            ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr','Mr'],inplace=True)\n\ndf_test['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Cpat','Sir','Don','Dona'],\n                           ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr','Mr'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:20.738460Z","iopub.execute_input":"2022-01-21T16:07:20.738670Z","iopub.status.idle":"2022-01-21T16:07:20.748398Z","shell.execute_reply.started":"2022-01-21T16:07:20.738643Z","shell.execute_reply":"2022-01-21T16:07:20.747588Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_train.groupby('Initial').mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:20.750766Z","iopub.execute_input":"2022-01-21T16:07:20.751270Z","iopub.status.idle":"2022-01-21T16:07:20.780616Z","shell.execute_reply.started":"2022-01-21T16:07:20.751225Z","shell.execute_reply":"2022-01-21T16:07:20.779829Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"- 여성과 관계있는 Miss, Mrs가 생존률이 높은 것을 볼 수 있다. ","metadata":{}},{"cell_type":"code","source":"df_train.groupby('Initial')['Survived'].mean().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:20.781842Z","iopub.execute_input":"2022-01-21T16:07:20.782615Z","iopub.status.idle":"2022-01-21T16:07:21.022586Z","shell.execute_reply.started":"2022-01-21T16:07:20.782573Z","shell.execute_reply":"2022-01-21T16:07:21.021638Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"- 이제 본격적으로 Null을 채울 것이다. null data를 채우는 방법은 정말 많이 존재한다. statistics를 활용하는 방법도 있고, null data가 없는 데이터를 기반으로 새로운 머신러닝 알고리즘을 만들어 예측해서 채워넣는 방식도 있다. 여기서는 statistics를 활용하는 방법을 사용할 것이다.\n- 여기서 statistics는 train data의 것을 의미한다. 우리는 언제나 test를 unseen으로 둔 상태로 놔둬야 하며, train에서 얻은 statistics를 기반으로 test의 null data를 채워줘야 한다.","metadata":{}},{"cell_type":"code","source":"df_train.groupby('Initial').mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.023867Z","iopub.execute_input":"2022-01-21T16:07:21.024091Z","iopub.status.idle":"2022-01-21T16:07:21.041493Z","shell.execute_reply.started":"2022-01-21T16:07:21.024065Z","shell.execute_reply":"2022-01-21T16:07:21.040915Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"- Age의 평균을 이용해 Null Value를 채우도록 하자. \n- pandas dataframe을 다룰 때에는 boolean array를 이용해 indexing하는 방법이 참으로 편리하다. \n- 아래 코드 첫 줄을 해석하자면, isnull()이면서 Initial이 Mr인 조건을 만족하는 row(탑승객)의 'Age' 값을 33으로 치환한다는 것이다.\n- loc + boolean + column을 사용해 값을 치환하는 방법은 자주 쓰이므로 꼭 익숙해지도록 하자. ","metadata":{}},{"cell_type":"code","source":"df_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Mr'),'Age'] = 33\ndf_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Mrs'),'Age'] = 36\ndf_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Master'),'Age'] = 5\ndf_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Miss'),'Age'] = 22\ndf_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Other'),'Age'] = 46\n\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Mr'),'Age'] = 33\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Mrs'),'Age'] = 36\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Master'),'Age'] = 5\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Miss'),'Age'] = 22\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Other'),'Age'] = 46","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.042577Z","iopub.execute_input":"2022-01-21T16:07:21.043194Z","iopub.status.idle":"2022-01-21T16:07:21.062312Z","shell.execute_reply.started":"2022-01-21T16:07:21.043151Z","shell.execute_reply":"2022-01-21T16:07:21.061556Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"- 여기선 간단하게 Null을 채웠지만, 좀 더 다양한 방법을 쓴 예시들이 다른 커널에 존재한다. \n- https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling 보면서 공부하자.\n- 이 외에도 다른 캐글러들의 커널을 보며 여러 참신한 아이디어를 살펴보자.","metadata":{}},{"cell_type":"markdown","source":"### 3.1.2 Fill Null in Embarked","metadata":{}},{"cell_type":"code","source":"print('Embarked has ', sum(df_train['Embarked'].isnull()), ' Null values')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.063622Z","iopub.execute_input":"2022-01-21T16:07:21.064240Z","iopub.status.idle":"2022-01-21T16:07:21.070104Z","shell.execute_reply.started":"2022-01-21T16:07:21.064207Z","shell.execute_reply":"2022-01-21T16:07:21.069108Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"- Embarked는 Null value가 2개이고, S에서 가장 많은 탑승객이 있었으므로, 간단하게 Null을 S로 채워보자. \n- dataframe의 fillna method를 이용하면 쉽게 채울 수 있다. 여기서 inplace=True로 하면 df_train에 fillna를 실제로 적용하게 된다. ","metadata":{}},{"cell_type":"code","source":"df_train['Embarked'].fillna('S', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.071329Z","iopub.execute_input":"2022-01-21T16:07:21.071840Z","iopub.status.idle":"2022-01-21T16:07:21.081949Z","shell.execute_reply.started":"2022-01-21T16:07:21.071795Z","shell.execute_reply":"2022-01-21T16:07:21.081102Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Change Age(continuous to categorical)\n- Age는 현재 continuous feature다. 이대로 써도 모델을 세울 수 있지만, Age를 몇 개의 group으로 나누어 category화 시켜줄 수도 있다. continuous를 categorical로 바꾸면 자칫 information loss가 생길 수도 있지만, 본 튜토리얼에서는 다양한 방법을 소개하는 것이 목적이므로 진행하도록 하자. \n- 방법은 여러가지가 있다. dataframe의 indexing 방법인 loc를 사용해 직접해 줄 수도 있고, 아니면 apply를 사용해 함수를 넣어줄 수도 있다. \n- 첫번째로 loc를 사용한 방법이다. loc는 자주 쓰게 되므로 그 사용법을 숙지하면 좋다. \n- 나이는 10살 간격으로 나눌 것이다. ","metadata":{}},{"cell_type":"code","source":"df_train['Age_cat'] = 0\ndf_train.loc[df_train['Age'] < 10, 'Age_cat'] = 0\ndf_train.loc[(10 <= df_train['Age']) & (df_train['Age'] < 20), 'Age_cat'] = 1\ndf_train.loc[(20 <= df_train['Age']) & (df_train['Age'] < 30), 'Age_cat'] = 2\ndf_train.loc[(30 <= df_train['Age']) & (df_train['Age'] < 40), 'Age_cat'] = 3\ndf_train.loc[(40 <= df_train['Age']) & (df_train['Age'] < 50), 'Age_cat'] = 4\ndf_train.loc[(50 <= df_train['Age']) & (df_train['Age'] < 60), 'Age_cat'] = 5\ndf_train.loc[(60 <= df_train['Age']) & (df_train['Age'] < 70), 'Age_cat'] = 6\ndf_train.loc[70 <= df_train['Age'], 'Age_cat'] = 7\n\ndf_test['Age_cat'] = 0\ndf_test.loc[df_train['Age'] < 10, 'Age_cat'] = 0\ndf_test.loc[(10 <= df_test['Age']) & (df_test['Age'] < 20), 'Age_cat'] = 1\ndf_test.loc[(20 <= df_test['Age']) & (df_test['Age'] < 30), 'Age_cat'] = 2\ndf_test.loc[(30 <= df_test['Age']) & (df_test['Age'] < 40), 'Age_cat'] = 3\ndf_test.loc[(40 <= df_test['Age']) & (df_test['Age'] < 50), 'Age_cat'] = 4\ndf_test.loc[(50 <= df_test['Age']) & (df_test['Age'] < 60), 'Age_cat'] = 5\ndf_test.loc[(60 <= df_test['Age']) & (df_test['Age'] < 70), 'Age_cat'] = 6\ndf_test.loc[70 <= df_test['Age'], 'Age_cat'] = 7","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.083227Z","iopub.execute_input":"2022-01-21T16:07:21.084088Z","iopub.status.idle":"2022-01-21T16:07:21.109139Z","shell.execute_reply.started":"2022-01-21T16:07:21.084052Z","shell.execute_reply":"2022-01-21T16:07:21.108417Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"- 두 번째로 간단한 함수를 만들어 apply 메소드에 넣어주는 방법이다. \n- 훨씬 수월하다. ","metadata":{}},{"cell_type":"code","source":"def category_age(x):\n    if x < 10:\n        return 0\n    elif x < 20:\n        return 1\n    elif x < 30:\n        return 2\n    elif x < 40:\n        return 3\n    elif x < 50:\n        return 4\n    elif x < 60:\n        return 5\n    elif x < 70:\n        return 6\n    else:\n        return 7\n    \ndf_train['Age_cat_2'] = df_train['Age'].apply(category_age)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.110290Z","iopub.execute_input":"2022-01-21T16:07:21.110669Z","iopub.status.idle":"2022-01-21T16:07:21.117103Z","shell.execute_reply.started":"2022-01-21T16:07:21.110640Z","shell.execute_reply":"2022-01-21T16:07:21.116163Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"- 두 가지 방법이 잘 적용됐다면, 둘 다 같은 결과를 내야한다. \n- 이를 확인하기 위해 Series 간 boolean 비교후 all() 메소드를 사용하자. all() 메소드는 모든 값이 True이면 True, 하나라도 False가 있으면 False를 준다.","metadata":{}},{"cell_type":"code","source":"print('1번 방법, 2번 방법 둘 다 같은 결과를 내면 True 줘야함 --> ', (df_train['Age_cat'] == df_train['Age_cat_2']).all())","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.118461Z","iopub.execute_input":"2022-01-21T16:07:21.120585Z","iopub.status.idle":"2022-01-21T16:07:21.129781Z","shell.execute_reply.started":"2022-01-21T16:07:21.120545Z","shell.execute_reply":"2022-01-21T16:07:21.128908Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"- 보다시피 True다. 둘 중 편한 걸 선택하며 된다. \n- 이제 중복되는 Age_cat 컬럼과 원래 컬럼 Age를 제거하자. ","metadata":{}},{"cell_type":"code","source":"df_train.drop(['Age', 'Age_cat_2'], axis=1, inplace=True)\ndf_test.drop(['Age'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.132488Z","iopub.execute_input":"2022-01-21T16:07:21.132929Z","iopub.status.idle":"2022-01-21T16:07:21.144903Z","shell.execute_reply.started":"2022-01-21T16:07:21.132894Z","shell.execute_reply":"2022-01-21T16:07:21.144231Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Change Initial, Embarked and Sex (string to numerical)\n- 현재 Initial은 Mr, Mrs, Miss, Master, Other 총 5개로 이루어져 있다. 이런 카테고리로 표현되어져 있는 데이터를 모델에 인풋으로 넣어줄 때 우리가 해야할 것으 먼저 컴퓨터가 인식할 수 있도록 수치화시켜야 한다. \n- map method를 가지고 간단히 할 수 있다. \n- 사전 순서대로 정리해 mapping 한다. ","metadata":{}},{"cell_type":"code","source":"df_train['Initial'] = df_train['Initial'].map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})\ndf_test['Initial'] = df_test['Initial'].map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.146016Z","iopub.execute_input":"2022-01-21T16:07:21.146362Z","iopub.status.idle":"2022-01-21T16:07:21.159531Z","shell.execute_reply.started":"2022-01-21T16:07:21.146335Z","shell.execute_reply":"2022-01-21T16:07:21.158837Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"- Embarked도 C, Q, S로 이루어져 있다. map을 이용해 바꿔보자. \n- 그러기 앞서서, 특정 column에 어떤 값들이 있는지 확인해보는 방법을 잠깐 살펴보자. 간단히 unique() 메소드를 쓰거나, value_counts()를 써서 count까지 보는 방법이 있다. ","metadata":{}},{"cell_type":"code","source":"df_train['Embarked'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.160543Z","iopub.execute_input":"2022-01-21T16:07:21.161084Z","iopub.status.idle":"2022-01-21T16:07:21.170711Z","shell.execute_reply.started":"2022-01-21T16:07:21.161048Z","shell.execute_reply":"2022-01-21T16:07:21.170006Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df_train['Embarked'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.171875Z","iopub.execute_input":"2022-01-21T16:07:21.172555Z","iopub.status.idle":"2022-01-21T16:07:21.185851Z","shell.execute_reply.started":"2022-01-21T16:07:21.172513Z","shell.execute_reply":"2022-01-21T16:07:21.185115Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"- 위 두 방법을 사용해 Embarked가 S, C, Q 세 가지로 이루어진 것을 볼 수 있다. 이제 map을 사용해보자. ","metadata":{}},{"cell_type":"code","source":"df_train['Embarked'] = df_train['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\ndf_test['Embarked'] = df_test['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.187039Z","iopub.execute_input":"2022-01-21T16:07:21.187404Z","iopub.status.idle":"2022-01-21T16:07:21.201533Z","shell.execute_reply.started":"2022-01-21T16:07:21.187375Z","shell.execute_reply":"2022-01-21T16:07:21.200840Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"- 한 번 Null이 사라졌는지 확인해보자. Embarked Column만 가져온 것은 하나의 pandas의 Series 객체이므로, isnull() 메소드를 사용해 Series의 값들이 null인지 아닌지에 대한 boolean 값을 얻을 수 있다. 그리고 이것에 any()를 사용해, True가 단 하나라도 있을 시(Null이 한 개라도 있을 시) True를 반환하게 된다. 우리는 Null을 모두 S로 바꿔주었으므로 False를 얻게 된다. ","metadata":{}},{"cell_type":"code","source":"df_train['Embarked'].isnull().any()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.202716Z","iopub.execute_input":"2022-01-21T16:07:21.203139Z","iopub.status.idle":"2022-01-21T16:07:21.215450Z","shell.execute_reply.started":"2022-01-21T16:07:21.203107Z","shell.execute_reply":"2022-01-21T16:07:21.214721Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"- Sex도 Female, Male로 이루어져 있다. map을 이용해 바꿔보자. ","metadata":{}},{"cell_type":"code","source":"df_train['Sex'] = df_train['Sex'].map({'female': 0, 'male': 1})\ndf_test['Sex'] = df_test['Sex'].map({'female': 0, 'male': 1})","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.216396Z","iopub.execute_input":"2022-01-21T16:07:21.217070Z","iopub.status.idle":"2022-01-21T16:07:21.225905Z","shell.execute_reply.started":"2022-01-21T16:07:21.217039Z","shell.execute_reply":"2022-01-21T16:07:21.225204Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"- 여지껏 고생했다. 이제 각 feature 간의 상관관계를 한 번 보려고 한다. 두 변수 간의 Pearson correlation을 구하면 (-1, 1) 사이의 값을 얻을 수 있다. -1로 갈수록 음의 상관관계, 1로 갈수록 양의 상관관계를 의미하며, 0은 상관관계가 없다는 것을 의미한다. 구하는 수식은 아래와 같다. \n- 우리는 여러 feature를 가지고 있으니 이를 하나의 matrix 형태로 보면 편할텐데, 이를 heatmap plot이라고 하며, dataframe의 corr() 메소드와 seaborn을 가지고 편하게 그릴 수 있다. ","metadata":{}},{"cell_type":"code","source":"heatmap_data = df_train[['Survived', 'Pclass', 'Sex', 'Fare', 'Embarked', 'FamilySize', 'Initial', 'Age_cat']]\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14, 12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(heatmap_data.astype(float).corr(), linewidths=0.1, vmax=1.0,\n           square=True, cmap=colormap, linecolor='white', annot=True, annot_kws={\"size\": 16})\n\ndel heatmap_data","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.226921Z","iopub.execute_input":"2022-01-21T16:07:21.227225Z","iopub.status.idle":"2022-01-21T16:07:21.906564Z","shell.execute_reply.started":"2022-01-21T16:07:21.227191Z","shell.execute_reply":"2022-01-21T16:07:21.905849Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"- 우리가 EDA에서 살펴봤듯이, Sex와 Pclass가 Survived에 상관관계가 어느 정도 있음을 볼 수 있다. \n- 생각보다 fare와 Embarked도 상관관계가 있음을 볼 수 있다. \n- 또한 우리가 여기서 얻을 수 있는 정보는 서로 강한 상관관계를 가지는 feature들이 없다는 것이다. \n- 이것은 우리가 모델을 학습시킬 때, 불필요한(redundant, superfluous) feature가 없다는 것을 의미한다. 1 또는 -1의 상관관계를 가진 feature A, B가 있다면, 우리가 얻을 수 있는 정보는 사실 하나일 것이다. \n- 이제 실제로 모델을 학습시키기 앞서서 data preprocessing(전처리)을 진행해보자. 거의 다 와가니까 힘내자!","metadata":{}},{"cell_type":"markdown","source":"## 3.4 One-hot encoding on Initial and Embarked\n- 수치화시킨 카테고리 데이터를 그대로 넣어도 되지만, 모델의 성능을 높이기 위해 one-hot encoding을 해 줄 수 있다. \n- 수치화는 간단히 Master == 0, Miss == 1, Mr == 2, Mrs == 3, Other == 4로 매핑해주는 것을 말한다. \n- One-hot encoding은 위 카테고리를 아래와 같이 (0, 1)로 이루어진 5차원의 벡터로 나타내는 것을 말한다. ","metadata":{}},{"cell_type":"markdown","source":"| |Initial_Master|Initial_Miss|Initial_Mr|Initial_Mrs|Initial_Other|\n|:---:|:---:|:---:|:---:|:---:|:---:|\n|Master|1|0|0|0|0|\n|Miss|0|1|0|0|0|\n|Mr|0|0|1|0|0|\n|Mrs|0|0|0|1|0|\n|Other|0|0|0|0|1|","metadata":{}},{"cell_type":"markdown","source":"- 위와 같은 작업을 직접 코딩할 수도 있지만, pandas의 get_dummies를 사용해 쉽게 해결할 수 있다. \n- 총 5개의 카테고리니, one-hot encoding을 하고 나면 새로운 5개의 column이 생겨난다. \n- Initial을 prefix로 두어서 구분이 쉽게 만들어 준다. ","metadata":{}},{"cell_type":"code","source":"df_train = pd.get_dummies(df_train, columns=['Initial'], prefix='Initial')\ndf_test = pd.get_dummies(df_test, columns=['Initial'], prefix='Initial')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.907540Z","iopub.execute_input":"2022-01-21T16:07:21.908232Z","iopub.status.idle":"2022-01-21T16:07:21.921186Z","shell.execute_reply.started":"2022-01-21T16:07:21.908194Z","shell.execute_reply":"2022-01-21T16:07:21.920336Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.922436Z","iopub.execute_input":"2022-01-21T16:07:21.922867Z","iopub.status.idle":"2022-01-21T16:07:21.940507Z","shell.execute_reply.started":"2022-01-21T16:07:21.922812Z","shell.execute_reply":"2022-01-21T16:07:21.939426Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"- 보다시피 오른쪽에 우리가 만들려고 했던 one-hot encoded columns가 생성된 것이 보인다. \n- Embarked에도 적용하자. Initial 때와 마찬가지로 one-hot encoding을 사용해 표현한다. ","metadata":{}},{"cell_type":"code","source":"df_train = pd.get_dummies(df_train, columns=['Embarked'], prefix='Embarked')\ndf_test = pd.get_dummies(df_test, columns=['Embarked'], prefix='Embarked')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.941946Z","iopub.execute_input":"2022-01-21T16:07:21.942235Z","iopub.status.idle":"2022-01-21T16:07:21.964059Z","shell.execute_reply.started":"2022-01-21T16:07:21.942193Z","shell.execute_reply":"2022-01-21T16:07:21.963189Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"- 아주 쉽게 one-hot encoding을 적용했다. \n- sklearn으로  Labelencoder + OneHotencoder 이용해도 one-hot encoding이 가능하다. \n- 다른 튜토리얼에서 한 번 써보자. 여기서는 get_dummies로 충분히 가능하기 때문에 get_dummies만으로 끝낸다.\n- 가끔 category가 100개가 넘어가는 경우가 있다. 이 때 one-hot encoding을 사용하면 column이 100개가 생겨, 학습 시 매우 버거울 경우가 있다. 이런 경우는 다른 방법을 사용하기도 하는데, 이는 다음에 한 번 다뤄보겠다. ","metadata":{}},{"cell_type":"markdown","source":"## 3.5 Drop columns\n- 고생했다. 이제 책상을 치울 때다. 필요한 columns만 남기고 다 지우자. ","metadata":{}},{"cell_type":"code","source":"df_train.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)\ndf_test.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.967007Z","iopub.execute_input":"2022-01-21T16:07:21.967271Z","iopub.status.idle":"2022-01-21T16:07:21.974421Z","shell.execute_reply.started":"2022-01-21T16:07:21.967243Z","shell.execute_reply":"2022-01-21T16:07:21.973495Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.975792Z","iopub.execute_input":"2022-01-21T16:07:21.976195Z","iopub.status.idle":"2022-01-21T16:07:21.993997Z","shell.execute_reply.started":"2022-01-21T16:07:21.976165Z","shell.execute_reply":"2022-01-21T16:07:21.993337Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:21.995061Z","iopub.execute_input":"2022-01-21T16:07:21.995464Z","iopub.status.idle":"2022-01-21T16:07:22.014144Z","shell.execute_reply.started":"2022-01-21T16:07:21.995379Z","shell.execute_reply":"2022-01-21T16:07:22.013238Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"- 보다시피, train의 Survived feature(target class)를 빼면, train, test 둘 다 같은 columns를 가진 걸 확인할 수 있다. ","metadata":{}},{"cell_type":"markdown","source":"##  4. Building machine learning model and prediction using the trained model\n- 이제 준비가 다 되었으니 sklearn을 사용해 본격적으로 머신러닝 모델을 만들어보자. ","metadata":{}},{"cell_type":"code","source":"#importing all the required ML packages\nfrom sklearn.ensemble import RandomForestClassifier  # 유명한 randomforestclassifier다. \nfrom sklearn import metrics  # 모델의 평가를 위해서 쓴다. \nfrom sklearn.model_selection import train_test_split  # training set을 쉽게 나눠주는 함수다. ","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:22.015243Z","iopub.execute_input":"2022-01-21T16:07:22.015461Z","iopub.status.idle":"2022-01-21T16:07:22.376434Z","shell.execute_reply.started":"2022-01-21T16:07:22.015433Z","shell.execute_reply":"2022-01-21T16:07:22.375608Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"- sklearn은 머신러닝의 처음부터 끝까지가 다 있다. feature engineering, preprocessing, 지도 학습 알고리즘, 비지도 학습 알고리즘, 모델 평가, 파이프라인 등 머신러닝에 관련된 모든 작업들이 손쉬운 인터페이스로 구현되어 있다. 데이터 분석 + 머신러닝을 하고싶다면, 이 라이브러리는 반드시 숙지해야 한다. \n- 파이썬 라이브러리를 활용한 머신러닝(Introduction to machine learning with Python) 책을 사서 공부하길 매우 추천한다. \n\n- 지금 타이타닉 문제는 target class(survived)가 있으며, target class는 0, 1로 이루어져 있으므로(binary) binary classification 문제다. \n- 우리가 지금 가지고 있는 train set의 survived를 제외한 input을 가지고 모델을 최적화시켜서 각 샘플(탑승객)의 생존유무를 판단하는 모델을 만들어낸다. \n- 그 후 모델이 학습하지 않았던 test set을 input으로 주어서 test set의 각 샘플(탑승객)의 생존 유무를 예측해본다. ","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Preparation - Split dataset into train, valid, test set\n- 가장 먼저, 학습에 쓰일 데이터와, target label(survived)를 분리한다. drop을 사용해 간단히 할 수 있다. ","metadata":{}},{"cell_type":"code","source":"X_train = df_train.drop('Survived', axis=1).values\ntarget_label = df_train['Survived'].values\nX_test = df_test.values","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:22.377607Z","iopub.execute_input":"2022-01-21T16:07:22.378304Z","iopub.status.idle":"2022-01-21T16:07:22.383661Z","shell.execute_reply.started":"2022-01-21T16:07:22.378266Z","shell.execute_reply":"2022-01-21T16:07:22.383132Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"- 보통 train, test만 언급되지만, 실제 좋은 모델을 만들기 위해서 valid set을 따로 만들어 모델을 평가한다. \n- 마치 축구대표팀이 팀훈련(train)을 하고 바로 월드컵(test)으로 나가는 것이 아니라, 팀훈련(train)을 한 다음 평가전(valid)를 거쳐 팀의 훈련 정도(학습정도)를 확인하고 월드컵(test)에 나가는 것과 비슷하다. \n- train_test_split을 사용해 쉽게 train 셋을 분리할 수 있다. ","metadata":{}},{"cell_type":"code","source":"X_tr,X_vld, y_tr, y_vld = train_test_split(X_train, target_label, test_size=0.3, random_state=2018)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:22.384585Z","iopub.execute_input":"2022-01-21T16:07:22.385306Z","iopub.status.idle":"2022-01-21T16:07:22.400617Z","shell.execute_reply.started":"2022-01-21T16:07:22.385276Z","shell.execute_reply":"2022-01-21T16:07:22.399794Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"- sklearn에서는 여러 머신러닝 알고리즘을 지원해준다. 열거하기엔 너무 많으므로, 직접 documentation에 들어가 보길 추천한다. http://scikit-learn.org/stable/supervised_learning.html#supervised-learning 여기에 들어가면 지원되는 알고리즘 수에 놀랄 것이다.\n- 본 튜토리얼에서는 랜덤포레스트 모델을 사용한다. \n- 랜덤포레스트는 결정트리기반 모델이며, 여러 결정 트리들을 앙상블한 모델이다. 더 구체적인 모델 설명은 여러 블로그들 참고하며 될 것이고, 추후 한 번 다뤄보자. \n- 각 머신러닝 알고리즘에는 여러 파라미터들이 있다. 랜더포레스트 분류기도 n_estimators, max_features, max_depth, min_samples_split, min_samples_leaf 등 여러 파라미터들이 존재한다. 이것들이 어떻게 세팅되냐에 따라 같은 데이터셋이라 하더라도 모델의 성능이 달라진다. \n- 파라미터 튜닝은 시간, 경험, 알고리즘에 대한 이해 등이 필요하다. 결국 많이 써봐야 모델도 잘 세울 수 있는 것이다. 그래서 캐글을 추천한다. 여러 데이터셋을 가지고 모델을 이리저리 써봐야 튜닝하는 감이 생길 것이니까.\n- 일단 지금은 튜토리얼이니 파라미터 튜닝은 잠시 제쳐두기로 하고, 기본 default 세팅으로 진행한다. \n- 모델 객체를 만들고, fit 메소드로 학습시킨다. \n- 그런 후 valid set input을 넣어주어 예측값(X_vld sample(탑승객)의 생존여부)을 얻는다. ","metadata":{}},{"cell_type":"markdown","source":"## 4.2 Model generation and prediction","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier()\nmodel.fit(X_tr, y_tr)\nprediction = model.predict(X_vld)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:22.402083Z","iopub.execute_input":"2022-01-21T16:07:22.402558Z","iopub.status.idle":"2022-01-21T16:07:22.589431Z","shell.execute_reply.started":"2022-01-21T16:07:22.402511Z","shell.execute_reply":"2022-01-21T16:07:22.588493Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"- 단 세 줄 만으로 모델을 세우고, 예측까지 해 보았다. \n- 자, 이제 모델의 성능으 한 번 살펴보자. ","metadata":{}},{"cell_type":"code","source":"print('총 {}명 중 {:.2f}% 정확도로 생존을 맞춤'.format(y_vld.shape[0], 100 * metrics.accuracy_score(prediction, y_vld)))","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:22.590967Z","iopub.execute_input":"2022-01-21T16:07:22.591381Z","iopub.status.idle":"2022-01-21T16:07:22.598539Z","shell.execute_reply.started":"2022-01-21T16:07:22.591336Z","shell.execute_reply":"2022-01-21T16:07:22.597588Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"- 아무런 파라미터 튜닝도 하지 않았는데 82%의 정확도가 나왔다. ","metadata":{}},{"cell_type":"markdown","source":"## 4.3 Feature importance\n- 학습된 모델은 feature importance를 갖게 되는데, 우리는 이것을 확인해 지금 만든 모델이 어떤 feature에 영향을 많이 받았는지 확인할 수 있다. \n- 쉽게 말해, 10 = 4x1 + 2x2 + 1x3을 생각하면, 우리는 x1이 결과값(10)에 큰 영향을 준다고 생각할 수 있다. feature importance는 4, 2, 1을 이야기하며, x1이 가장 큰 값(4)을 가짐로, 이 모델에 가장 큰 영향을 미친다고 말할 수 있다. \n\n- 학습된 모델은 기본적으로 feature importances를 가지고 있어서 쉽게 그 수치를 읽을 수 있다. \n- pandas series를 이용하면 쉽게 sorting을 하여 그래프를 그릴 수 있다. ","metadata":{}},{"cell_type":"code","source":"from pandas import Series\n\nfeature_importance = model.feature_importances_\nSeries_feat_imp = Series(feature_importance, index=df_test.columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:22.599727Z","iopub.execute_input":"2022-01-21T16:07:22.599988Z","iopub.status.idle":"2022-01-21T16:07:22.622247Z","shell.execute_reply.started":"2022-01-21T16:07:22.599958Z","shell.execute_reply":"2022-01-21T16:07:22.621632Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 8))\nSeries_feat_imp.sort_values(ascending=True).plot.barh()\nplt.xlabel('Feature importance')\nplt.ylabel('Feature')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:22.624121Z","iopub.execute_input":"2022-01-21T16:07:22.624667Z","iopub.status.idle":"2022-01-21T16:07:22.901131Z","shell.execute_reply.started":"2022-01-21T16:07:22.624624Z","shell.execute_reply":"2022-01-21T16:07:22.900441Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"- 우리가 얻은 모델에서는 Fare가 가장 큰 영향력을 가지며, 그 뒤로 Initial_2, Age_cat, Pclass가 차례로 중요도를 가진다. \n- 사실 feature importance는 지금 모델에서의 importance를 나타낸다. 만약 다른 모델을 사용하게 된다면 feature impotance가 다르게 나올 수 있다. \n- 이 feature importance를 보고 실제로 Fare가 중요한 feature일 수 있다고 판단을 내릴 수는 있지만, 이것은 결국 모델에 귀속하는 하나의 결론이므로 통계적으로 좀 더 살펴보긴 해야한다. \n- feature importance를 가지고 좀 더 정확도가 높은 모델을 얻기 위해 feature selection을 할 수도 있고, 더 빠른 모델을 위해 feature 제거를 할 수 있다. ","metadata":{}},{"cell_type":"markdown","source":"## 4.4 Prediction on Test set\n- 이제 모델이 학습하지 않았던 (보지 않았던) 테스트셋을 모델에 주어서, 생존여부를 예측해보자. \n- 이 결과는 실제로 submission(제출용)이므로 결과는 leaderboard에서 확인할 수 있다. \n- 캐글에서 준 파일, gender_submission.csv 파일을 읽어서 제출 준비를 하자. ","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('../input/titanic/gender_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:22.902558Z","iopub.execute_input":"2022-01-21T16:07:22.903006Z","iopub.status.idle":"2022-01-21T16:07:22.911462Z","shell.execute_reply.started":"2022-01-21T16:07:22.902956Z","shell.execute_reply":"2022-01-21T16:07:22.910457Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:22.912588Z","iopub.execute_input":"2022-01-21T16:07:22.912916Z","iopub.status.idle":"2022-01-21T16:07:22.924163Z","shell.execute_reply.started":"2022-01-21T16:07:22.912882Z","shell.execute_reply":"2022-01-21T16:07:22.923295Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"- 이제 testset에 대해 예측을 하고, 결과를 csv 파일로 저장해보자. ","metadata":{}},{"cell_type":"code","source":"prediction = model.predict(X_test)\nsubmission['Survived'] = prediction","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:22.927086Z","iopub.execute_input":"2022-01-21T16:07:22.927318Z","iopub.status.idle":"2022-01-21T16:07:22.953498Z","shell.execute_reply.started":"2022-01-21T16:07:22.927290Z","shell.execute_reply":"2022-01-21T16:07:22.952688Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('./my_first_submisson.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T16:07:22.954785Z","iopub.execute_input":"2022-01-21T16:07:22.955581Z","iopub.status.idle":"2022-01-21T16:07:22.961796Z","shell.execute_reply.started":"2022-01-21T16:07:22.955539Z","shell.execute_reply":"2022-01-21T16:07:22.961217Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"- 이제 캐글에 제출해보자.","metadata":{}},{"cell_type":"markdown","source":"## 5. Conclusion\n- 정말 수고했다. titanic dataset을 가지고 data science를 경험해보았다. \n- 이걸로 끝이 아니다. 앞으로 배울 것이 너무나 무궁무진하다. \n- 좀 더 참신한 feature engineering, 머신 러닝 모델 hyperparameter tuning, ensembing 등, 무궁무진하다. \n- 꾸준히 커널을 공부하면 실력이 꾸준히 늘 것이다. \n- 포기하지 말고 재밌게 하면 된다.\n- 본 튜토리얼은 https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python, https://www.kaggle.com/startupsci/titanic-data-science-solutions, https://www.kaggle.com/ash316/eda-to-prediction-dietanic, https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling 을 참고해 만들었다. 공유해주신 캐글러께 감사드린다. ","metadata":{}}]}